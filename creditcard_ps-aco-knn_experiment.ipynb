{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T12:22:29.852432Z",
     "start_time": "2025-09-07T12:22:29.835039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"Raw:\", df[\"Class\"].value_counts().to_dict())   # {0: 284315, 1: 492}\n",
    "y = 1 - df[\"Class\"].values\n",
    "print(\"After flip:\", {0: int((y==0).sum()), 1: int((y==1).sum())})  # {0: 492, 1: 284315}\n"
   ],
   "id": "1785513cdf19105d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: {0.0: 4410, 1.0: 2}\n",
      "After flip: {0: 2, 1: 4410}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# File: creditcard_ps_vs_baseline.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    average_precision_score, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import networkx as nx\n",
    "\n",
    "# =========================\n",
    "# 1) Load creditcard data\n",
    "# =========================\n",
    "# Expecting: data/creditcard.csv with columns:\n",
    "# Time, V1..V28, Amount, Class (1=fraud minority, 0=legit majority)\n",
    "df = pd.read_csv(os.path.join(\"data\", \"creditcard.csv\"))\n",
    "\n",
    "# Sanity check: show raw distribution\n",
    "# print(\"Raw Class counts:\", df[\"Class\"].value_counts().to_dict())\n",
    "\n",
    "# Our ACO code assumes: 0 = minority, 1 = majority\n",
    "# In creditcard, Class==1 is minority (fraud). Flip it:\n",
    "# new_y = 0 if fraud, 1 if normal\n",
    "y_raw = df[\"Class\"].values\n",
    "y = 1 - y_raw  # fraud(1)->0, normal(0)->1\n",
    "\n",
    "# features = everything except Class\n",
    "features = [c for c in df.columns if c != \"Class\"]\n",
    "X = df[features].values.astype(np.float64)\n",
    "\n",
    "# =========================\n",
    "# 2) Train/Test split + scale (fit scaler on TRAIN only)\n",
    "# =========================\n",
    "X_train_raw, X_test_raw, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, np.arange(len(df)),\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) ACO on kNN graph (TRAIN only), then infer PS for TEST\n",
    "# =========================\n",
    "def run_aco_with_knn(\n",
    "    X_train, y_train, X_test,\n",
    "    k=5, n_ants=50, n_iter=30, rho=0.1, rng_seed=42,\n",
    "    alpha_edge=1.0, beta_phero=1.0, tau=1.0,\n",
    "    max_steps=100, eps=1e-12\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Pheromone-Score (PS) via kNN graph + ACO on TRAIN only (no leakage),\n",
    "    then estimate PS for TEST by neighbor-weighted averaging.\n",
    "\n",
    "    Assumptions:\n",
    "      - X_train / X_test are already standardized (e.g., StandardScaler fit on train).\n",
    "\n",
    "    y convention:\n",
    "      - 0 = minority, 1 = majority\n",
    "\n",
    "    Returns:\n",
    "      pheromone_train_scaled (np.ndarray): PS in [0,1] for TRAIN nodes (shape: [n_train]).\n",
    "      pheromone_test_scaled  (np.ndarray): PS estimate in [0,1] for TEST nodes  (shape: [n_test]).\n",
    "    \"\"\"\n",
    "    # kNN on TRAIN\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(X_train)\n",
    "    dist_train, ind_train = nbrs.kneighbors(X_train)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    n_tr = len(X_train)\n",
    "    for i in range(n_tr):\n",
    "        G.add_node(i, label=int(y_train[i]), pheromone=0.0)\n",
    "\n",
    "    # build weighted edges using returned distances (avoid recomputing norms)\n",
    "    for i, neigh in enumerate(ind_train):\n",
    "        for jj, j in enumerate(neigh):\n",
    "            if i == j:\n",
    "                continue\n",
    "            d_ij = dist_train[i, jj]\n",
    "            w = 1.0 / (1.0 + d_ij)\n",
    "            if G.has_edge(i, j):\n",
    "                G[i][j]['weight'] = max(G[i][j]['weight'], w)\n",
    "            else:\n",
    "                G.add_edge(i, j, weight=w)\n",
    "\n",
    "    majority_nodes = [i for i in range(n_tr) if y_train[i] == 1]\n",
    "    minority_nodes = set([i for i in range(n_tr) if y_train[i] == 0])\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # ACO loop\n",
    "    for _ in range(n_iter):\n",
    "        for _ in range(n_ants):\n",
    "            current = rng.choice(majority_nodes)\n",
    "            visited = [current]\n",
    "            steps = 0\n",
    "\n",
    "            while steps < max_steps:\n",
    "                neighs = [n for n in G.neighbors(current) if n not in visited]\n",
    "                if not neighs:\n",
    "                    break\n",
    "\n",
    "                # move probability ∝ (edge_weight^alpha_edge) * ((1+pheromone)^beta_phero)\n",
    "                raw = []\n",
    "                for n in neighs:\n",
    "                    e = G[current][n]['weight'] ** alpha_edge\n",
    "                    p = (1.0 + G.nodes[n]['pheromone']) ** beta_phero\n",
    "                    raw.append(e * p)\n",
    "                raw = np.array(raw, dtype=float)\n",
    "\n",
    "                # temperature-scaled softmax for stability/controllability\n",
    "                logits = raw / max(tau, eps)\n",
    "                logits -= logits.max()\n",
    "                probs = np.exp(logits)\n",
    "                s = probs.sum()\n",
    "                if s <= eps:\n",
    "                    break\n",
    "                probs /= s\n",
    "\n",
    "                next_node = rng.choice(neighs, p=probs)\n",
    "                visited.append(next_node)\n",
    "                current = next_node\n",
    "                steps += 1\n",
    "\n",
    "                if current in minority_nodes:\n",
    "                    # deposit pheromone along the path\n",
    "                    for idx in visited:\n",
    "                        G.nodes[idx]['pheromone'] += 1.0\n",
    "                    break\n",
    "\n",
    "        # evaporation\n",
    "        for n in G.nodes:\n",
    "            G.nodes[n]['pheromone'] *= (1.0 - rho)\n",
    "\n",
    "    # normalize PS on TRAIN\n",
    "    pheromone_train = np.array([G.nodes[i]['pheromone'] for i in range(n_tr)], dtype=float)\n",
    "    scaler = MinMaxScaler().fit(pheromone_train.reshape(-1, 1))\n",
    "    pheromone_train_scaled = scaler.transform(pheromone_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # estimate PS for TEST via inverse-distance weighted average of TRAIN neighbors\n",
    "    dist_te, ind_te = nbrs.kneighbors(X_test, n_neighbors=k)\n",
    "    w_te = 1.0 / (eps + dist_te)\n",
    "    w_te = w_te / (w_te.sum(axis=1, keepdims=True) + eps)\n",
    "    pheromone_test_scaled = (w_te * pheromone_train_scaled[ind_te]).sum(axis=1)\n",
    "\n",
    "    return pheromone_train_scaled, pheromone_test_scaled\n",
    "\n",
    "\n",
    "# --------- Optional speed control for graph build ----------\n",
    "# For very large training sets, you can cap the data used for the kNN graph\n",
    "# and still infer PS for the full Train/Test by kNN to that subset.\n",
    "max_train_for_graph = 60000  # adjust based on your machine\n",
    "if len(X_train) > max_train_for_graph:\n",
    "    rng = np.random.default_rng(42)\n",
    "    keep = rng.choice(len(X_train), size=max_train_for_graph, replace=False)\n",
    "    X_train_graph = X_train[keep]\n",
    "    y_train_graph = y_train[keep]\n",
    "\n",
    "    # Run ACO on the subset\n",
    "    ps_train_sub, ps_test_est = run_aco_with_knn(\n",
    "        X_train_graph, y_train_graph, X_test,\n",
    "        k=10, n_ants=75, n_iter=25, rho=0.1, rng_seed=42,\n",
    "        alpha_edge=1.0, beta_phero=1.0, tau=1.0,\n",
    "        max_steps=50\n",
    "    )\n",
    "\n",
    "    # Now estimate PS for ALL train points by kNN to the subset\n",
    "    nbrs_sub = NearestNeighbors(n_neighbors=10, metric='euclidean').fit(X_train_graph)\n",
    "    dist_tr_all, ind_tr_all = nbrs_sub.kneighbors(X_train, n_neighbors=10)\n",
    "    w_tr_all = 1.0 / (1e-12 + dist_tr_all)\n",
    "    w_tr_all = w_tr_all / (w_tr_all.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    ps_train_all = (w_tr_all * ps_train_sub[ind_tr_all]).sum(axis=1)\n",
    "\n",
    "    pheromone_train_scaled = ps_train_all\n",
    "    pheromone_test_scaled  = ps_test_est\n",
    "else:\n",
    "    pheromone_train_scaled, pheromone_test_scaled = run_aco_with_knn(\n",
    "        X_train, y_train, X_test,\n",
    "        k=10, n_ants=75, n_iter=25, rho=0.1, rng_seed=42,\n",
    "        alpha_edge=1.0, beta_phero=1.0, tau=1.0,\n",
    "        max_steps=50\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Build PS-weighted resample for RF\n",
    "# =========================\n",
    "def build_ps_weighted_resample(X_tr, y_tr, ps_tr,\n",
    "                               target_ratio=1.0,\n",
    "                               alpha=0.1, beta=0.9,\n",
    "                               rng_seed=42):\n",
    "    \"\"\"\n",
    "    Create a resampled training set where minority (label=0) samples are\n",
    "    oversampled with probability ∝ (alpha + beta*PS).\n",
    "\n",
    "    target_ratio: desired minority/majority count in the resampled set (1.0 → balanced)\n",
    "\n",
    "    Returns: X_res, y_res\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    idx_min = np.where(y_tr == 0)[0]\n",
    "    idx_maj = np.where(y_tr == 1)[0]\n",
    "\n",
    "    n_min = len(idx_min)\n",
    "    n_maj = len(idx_maj)\n",
    "\n",
    "    # how many minority samples we want\n",
    "    n_min_target = int(target_ratio * n_maj)\n",
    "\n",
    "    # sampling probabilities for minority based on PS\n",
    "    ps_min = ps_tr[idx_min]\n",
    "    probs = alpha + beta * ps_min\n",
    "    probs = np.maximum(probs, 1e-12)\n",
    "    probs = probs / probs.sum()\n",
    "\n",
    "    # sample with replacement to reach target\n",
    "    if n_min_target <= n_min:\n",
    "        chosen_min = rng.choice(idx_min, size=n_min_target, replace=False, p=probs)\n",
    "    else:\n",
    "        chosen_min = rng.choice(idx_min, size=n_min_target, replace=True,  p=probs)\n",
    "\n",
    "    # keep all majority (or you can also bootstrap majority if you prefer)\n",
    "    chosen_maj = idx_maj\n",
    "\n",
    "    idx_res = np.concatenate([chosen_min, chosen_maj])\n",
    "    rng.shuffle(idx_res)\n",
    "\n",
    "    return X_tr[idx_res], y_tr[idx_res]\n",
    "\n",
    "\n",
    "X_tr_ps, y_tr_ps = build_ps_weighted_resample(\n",
    "    X_train, y_train, pheromone_train_scaled,\n",
    "    target_ratio=1.0,   # make minority ~= majority\n",
    "    alpha=0.1, beta=0.9,\n",
    "    rng_seed=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5) Train & evaluate\n",
    "# =========================\n",
    "def prob_of_class0(model, X):\n",
    "    \"\"\"Return P(y=0) robustly using model.classes_ ordering.\"\"\"\n",
    "    cls = list(model.classes_)\n",
    "    idx0 = cls.index(0)\n",
    "    return model.predict_proba(X)[:, idx0]\n",
    "\n",
    "def evaluate(y_true, y_prob_class0, thresh=0.5):\n",
    "    \"\"\"\n",
    "    y_prob_class0: predicted probability for class 0 (fraud, positive)\n",
    "    Decision rule: ŷ = 0 if P(class=0) >= thresh else 1\n",
    "    \"\"\"\n",
    "    y_pred = np.where(y_prob_class0 >= thresh, 0, 1)\n",
    "    y_pos = (y_true == 0).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"F1\":        f1_score(y_true, y_pred, pos_label=0),\n",
    "        \"Precision\": precision_score(y_true, y_pred, pos_label=0, zero_division=0),\n",
    "        \"Recall\":    recall_score(y_true, y_pred, pos_label=0),\n",
    "        \"Accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"AUC_PR\":    average_precision_score(y_pos, y_prob_class0),\n",
    "        \"ROC_AUC\":   roc_auc_score(y_pos, y_prob_class0),\n",
    "    }\n",
    "\n",
    "# Baseline RF (no class_weight, no PS)\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_base.fit(X_train, y_train)\n",
    "proba_base = prob_of_class0(rf_base, X_test)\n",
    "metrics_base = evaluate(y_test, proba_base, thresh=0.5)\n",
    "\n",
    "# PS-weighted resampled RF\n",
    "rf_ps = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_ps.fit(X_tr_ps, y_tr_ps)\n",
    "proba_ps = prob_of_class0(rf_ps, X_test)\n",
    "metrics_ps = evaluate(y_test, proba_ps, thresh=0.5)\n",
    "\n",
    "# =========================\n",
    "# 6) Report\n",
    "# =========================\n",
    "def pretty(d):\n",
    "    return \" | \".join(f\"{k}: {v:.4f}\" for k, v in d.items())\n",
    "\n",
    "print(\"\\n=== Results (positive class = fraud → label 0) ===\")\n",
    "print(\"Baseline RF           :\", pretty(metrics_base))\n",
    "print(\"RF + PS-weighted SMPL :\", pretty(metrics_ps))\n"
   ],
   "id": "df226d41cfb23b0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lightweight ACO+kNN PS on creditcard dataset with minimal compute.\n",
    "- 0 = minority (fraud, positive), 1 = majority (normal)\n",
    "- Strategies: baseline RF, RF + PS(sample_weight), RF + PS-guided resampling\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    average_precision_score, roc_auc_score\n",
    ")\n",
    "import networkx as nx\n",
    "\n",
    "# =========================\n",
    "# 0) Load data + flip labels\n",
    "# =========================\n",
    "df = pd.read_csv(os.path.join(\"data\", \"creditcard.csv\"))\n",
    "# creditcard: Class==1 (fraud), Class==0 (normal)\n",
    "# Convention here: 0 = minority (fraud, positive), 1 = majority\n",
    "y = 1 - df[\"Class\"].values\n",
    "X = df.drop(columns=[\"Class\"]).values.astype(np.float64)\n",
    "\n",
    "# =========================\n",
    "# 1) Configuration (LIGHT MODE)\n",
    "# =========================\n",
    "N_FOLDS = 3                     # fewer than 5 for lighter runs\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Subset of Train used to build ACO graph in each fold:\n",
    "# - include all minority samples\n",
    "# - cap majority samples at cap_maj (undersample)\n",
    "cap_maj_for_graph_per_fold = 35000  # lower this if computation is heavy\n",
    "k_values  = [5, 10, 20, 30]         # small grid\n",
    "n_ants    = 100                     # fewer ants\n",
    "n_iter    = 50                      # fewer iterations\n",
    "rho       = 0.1\n",
    "alpha_edge = 1.0\n",
    "beta_phero = 1.0\n",
    "tau        = 1.0\n",
    "max_steps_scale = 20                # max_steps = max_steps_scale * k\n",
    "\n",
    "# RF configuration\n",
    "rf_n_estimators = 150\n",
    "rf_random_state = 42\n",
    "\n",
    "# PS-guided resampling\n",
    "target_ratio = 1.0  # target minority/majority ratio (≈ balanced)\n",
    "alpha = 0.1         # w_i ∝ alpha + beta * PS\n",
    "beta  = 0.9\n",
    "\n",
    "# =========================\n",
    "# 2) Helper functions\n",
    "# =========================\n",
    "def evaluate_pos0(y_true, y_prob_class0, thresh=0.5):\n",
    "    \"\"\"Metrics with positive class = 0 (fraud). y_prob_class0 = P(y=0).\"\"\"\n",
    "    y_pred = np.where(y_prob_class0 >= thresh, 0, 1)\n",
    "    y_pos = (y_true == 0).astype(int)\n",
    "    return {\n",
    "        \"F1\":        f1_score(y_true, y_pred, pos_label=0),\n",
    "        \"Precision\": precision_score(y_true, y_pred, pos_label=0, zero_division=0),\n",
    "        \"Recall\":    recall_score(y_true, y_pred, pos_label=0),\n",
    "        \"Accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"AUC_PR\":    average_precision_score(y_pos, y_prob_class0),\n",
    "        \"ROC_AUC\":   roc_auc_score(y_pos, y_prob_class0),\n",
    "    }\n",
    "\n",
    "def pretty(d):\n",
    "    return \" | \".join(f\"{k}: {v:.4f}\" for k, v in d.items())\n",
    "\n",
    "def ps_guided_resample(X_tr, y_tr, ps_tr, target_ratio=1.0, alpha=0.1, beta=0.9, seed=42):\n",
    "    \"\"\"Oversample minority (0) with probabilities ∝ alpha + beta*PS to reach target_ratio.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx_min = np.where(y_tr == 0)[0]\n",
    "    idx_maj = np.where(y_tr == 1)[0]\n",
    "    n_maj = len(idx_maj)\n",
    "    n_min_target = int(target_ratio * n_maj)\n",
    "    ps_min = ps_tr[idx_min]\n",
    "    probs = alpha + beta * ps_min\n",
    "    probs = np.maximum(probs, 1e-12)\n",
    "    probs = probs / probs.sum()\n",
    "    if n_min_target <= len(idx_min):\n",
    "        chosen_min = rng.choice(idx_min, size=n_min_target, replace=False, p=probs)\n",
    "    else:\n",
    "        chosen_min = rng.choice(idx_min, size=n_min_target, replace=True, p=probs)\n",
    "    idx_new = np.concatenate([idx_maj, chosen_min])\n",
    "    rng.shuffle(idx_new)\n",
    "    return X_tr[idx_new], y_tr[idx_new]\n",
    "\n",
    "# =========================\n",
    "# 3) ACO on kNN (LIGHT): precompute once per fold, reuse for different k via k_max\n",
    "# =========================\n",
    "def run_aco_with_knn_light(X_tr_graph, y_tr_graph, X_te,\n",
    "                           k, n_ants, n_iter, rho,\n",
    "                           alpha_edge, beta_phero, tau,\n",
    "                           max_steps, eps=1e-12,\n",
    "                           precomp=None):\n",
    "    \"\"\"\n",
    "    Light ACO on a pre-capped graph subset.\n",
    "    If precomp contains a fitted NearestNeighbors with k_max, reuses it and slices to k.\n",
    "    \"\"\"\n",
    "    # Fit or reuse neighbors (k_max)\n",
    "    if precomp is None or \"nbrs\" not in precomp or precomp[\"k_max\"] < k:\n",
    "        k_max = max(k, 40)  # build a single model with larger k to slice from\n",
    "        nbrs = NearestNeighbors(n_neighbors=k_max, metric='euclidean').fit(X_tr_graph)\n",
    "        dist_tr_all, ind_tr_all = nbrs.kneighbors(X_tr_graph)\n",
    "        precomp = {\"nbrs\": nbrs, \"dist_tr_all\": dist_tr_all, \"ind_tr_all\": ind_tr_all, \"k_max\": k_max}\n",
    "    else:\n",
    "        k_max = precomp[\"k_max\"]\n",
    "        nbrs = precomp[\"nbrs\"]\n",
    "        dist_tr_all = precomp[\"dist_tr_all\"]\n",
    "        ind_tr_all = precomp[\"ind_tr_all\"]\n",
    "\n",
    "    # Slice first-k\n",
    "    dist_train = dist_tr_all[:, :k]\n",
    "    ind_train  = ind_tr_all[:, :k]\n",
    "\n",
    "    # Build graph\n",
    "    G = nx.Graph()\n",
    "    n_tr = len(X_tr_graph)\n",
    "    for i in range(n_tr):\n",
    "        G.add_node(i, label=int(y_tr_graph[i]), pheromone=0.0)\n",
    "\n",
    "    for i, neigh in enumerate(ind_train):\n",
    "        for jj, j in enumerate(neigh):\n",
    "            if i == j:\n",
    "                continue\n",
    "            d_ij = dist_train[i, jj]\n",
    "            w = 1.0 / (1.0 + d_ij)\n",
    "            if G.has_edge(i, j):\n",
    "                G[i][j]['weight'] = max(G[i][j]['weight'], w)\n",
    "            else:\n",
    "                G.add_edge(i, j, weight=w)\n",
    "\n",
    "    majority_nodes = [i for i in range(n_tr) if y_tr_graph[i] == 1]\n",
    "    minority_nodes = set([i for i in range(n_tr) if y_tr_graph[i] == 0])\n",
    "\n",
    "    rng = np.random.default_rng(123)\n",
    "    for _ in range(n_iter):\n",
    "        for _ in range(n_ants):\n",
    "            current = rng.choice(majority_nodes)\n",
    "            visited = [current]\n",
    "            steps = 0\n",
    "            while steps < max_steps:\n",
    "                neighs = [n for n in G.neighbors(current) if n not in visited]\n",
    "                if not neighs:\n",
    "                    break\n",
    "                raw = []\n",
    "                for n in neighs:\n",
    "                    e = G[current][n]['weight'] ** alpha_edge\n",
    "                    p = (1.0 + G.nodes[n]['pheromone']) ** beta_phero\n",
    "                    raw.append(e * p)\n",
    "                raw = np.array(raw, dtype=float)\n",
    "                logits = raw / max(tau, eps)\n",
    "                logits -= logits.max()\n",
    "                probs = np.exp(logits)\n",
    "                s = probs.sum()\n",
    "                if s <= eps:\n",
    "                    break\n",
    "                probs /= s\n",
    "                next_node = rng.choice(neighs, p=probs)\n",
    "                visited.append(next_node)\n",
    "                current = next_node\n",
    "                steps += 1\n",
    "                if current in minority_nodes:\n",
    "                    for idx in visited:\n",
    "                        G.nodes[idx]['pheromone'] += 1.0\n",
    "                    break\n",
    "        for n in G.nodes:\n",
    "            G.nodes[n]['pheromone'] *= (1.0 - rho)\n",
    "\n",
    "    # Normalize PS on TRAIN(subset)\n",
    "    pheromone_train = np.array([G.nodes[i]['pheromone'] for i in range(n_tr)], dtype=float)\n",
    "    scaler = MinMaxScaler().fit(pheromone_train.reshape(-1, 1))\n",
    "    phero_train_scaled = scaler.transform(pheromone_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Infer PS for TEST w.r.t. graph subset\n",
    "    dist_te_all, ind_te_all = precomp[\"nbrs\"].kneighbors(X_te, n_neighbors=k)\n",
    "    w_te = 1.0 / (1e-12 + dist_te_all)\n",
    "    w_te = w_te / (w_te.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    phero_test_scaled = (w_te * phero_train_scaled[ind_te_all]).sum(axis=1)\n",
    "\n",
    "    return phero_train_scaled, phero_test_scaled, precomp\n",
    "\n",
    "# =========================\n",
    "# 4) Cross-validation split + per-fold graph subset\n",
    "# =========================\n",
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "folds = []\n",
    "for f, (tr_idx, te_idx) in enumerate(cv.split(X, y)):\n",
    "    X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "    scaler = StandardScaler().fit(X_tr_raw)\n",
    "    X_tr = scaler.transform(X_tr_raw)\n",
    "    X_te = scaler.transform(X_te_raw)\n",
    "\n",
    "    # Build lighter graph subset: all minority + capped majority\n",
    "    idx_min = np.where(y_tr == 0)[0]\n",
    "    idx_maj = np.where(y_tr == 1)[0]\n",
    "\n",
    "    if len(idx_maj) > cap_maj_for_graph_per_fold:\n",
    "        rng = np.random.default_rng(SEED + f)\n",
    "        idx_maj_cap = rng.choice(idx_maj, size=cap_maj_for_graph_per_fold, replace=False)\n",
    "    else:\n",
    "        idx_maj_cap = idx_maj\n",
    "\n",
    "    keep_idx = np.concatenate([idx_min, idx_maj_cap])\n",
    "    X_tr_graph = X_tr[keep_idx]\n",
    "    y_tr_graph = y_tr[keep_idx]\n",
    "\n",
    "    folds.append({\n",
    "        \"X_tr\": X_tr, \"y_tr\": y_tr, \"X_te\": X_te, \"y_te\": y_te,\n",
    "        \"X_tr_graph\": X_tr_graph, \"y_tr_graph\": y_tr_graph\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 5) Precompute PS for each fold, for a small set of k\n",
    "# =========================\n",
    "# We reuse a neighbor model with k_max and slice to any k <= k_max\n",
    "k_max = max(k_values)\n",
    "ps_cache = defaultdict(dict)  # ps_cache[f][k] = (ps_train_full, ps_test)\n",
    "\n",
    "for f, pack in enumerate(folds):\n",
    "    X_tr, y_tr = pack[\"X_tr\"], pack[\"y_tr\"]\n",
    "    X_te, y_te = pack[\"X_te\"], pack[\"y_te\"]\n",
    "    X_tr_g, y_tr_g = pack[\"X_tr_graph\"], pack[\"y_tr_graph\"]\n",
    "\n",
    "    precomp = None\n",
    "    for k in sorted(k_values):\n",
    "        # Run light ACO on subset\n",
    "        phero_tr_sub, phero_te, precomp = run_aco_with_knn_light(\n",
    "            X_tr_g, y_tr_g, X_te,\n",
    "            k=k, n_ants=n_ants, n_iter=n_iter, rho=rho,\n",
    "            alpha_edge=alpha_edge, beta_phero=beta_phero, tau=tau,\n",
    "            max_steps=max_steps_scale * k,\n",
    "            precomp=precomp\n",
    "        )\n",
    "        # Map PS back to FULL train via kNN-to-subset (10-NN)\n",
    "        nbrs_sub = NearestNeighbors(n_neighbors=min(10, len(X_tr_g)), metric='euclidean').fit(X_tr_g)\n",
    "        dist_tr_all, ind_tr_all = nbrs_sub.kneighbors(X_tr, n_neighbors=min(10, len(X_tr_g)))\n",
    "        w_tr_all = 1.0 / (1e-12 + dist_tr_all)\n",
    "        w_tr_all = w_tr_all / (w_tr_all.sum(axis=1, keepdims=True) + 1e-12)\n",
    "        phero_tr_full = (w_tr_all * phero_tr_sub[ind_tr_all]).sum(axis=1)\n",
    "\n",
    "        ps_cache[f][k] = (phero_tr_full, phero_te)\n",
    "\n",
    "# =========================\n",
    "# 6) Train/Evaluate (3 strategies)\n",
    "# =========================\n",
    "def run_fold_strategies(fpack, k, rf_n_estimators):\n",
    "    X_tr, y_tr = fpack[\"X_tr\"], fpack[\"y_tr\"]\n",
    "    X_te, y_te = fpack[\"X_te\"], fpack[\"y_te\"]\n",
    "    phero_tr, phero_te = ps_cache[fpack[\"id\"]][k]\n",
    "\n",
    "    # 6.1 Baseline RF (without PS)\n",
    "    rf_base = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=rf_random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_base.fit(X_tr, y_tr)\n",
    "    proba0_base = rf_base.predict_proba(X_te)[:, list(rf_base.classes_).index(0)]\n",
    "    met_base = evaluate_pos0(y_te, proba0_base)\n",
    "\n",
    "    # 6.2 RF + PS as feature + sample_weight\n",
    "    X_tr_w = np.column_stack([X_tr, phero_tr])\n",
    "    X_te_w = np.column_stack([X_te, phero_te])\n",
    "    weights = np.where(y_tr == 0, 1.0 + 2.0 * phero_tr, 1.0 + 0.5 * phero_tr)\n",
    "    rf_w = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        random_state=rf_random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_w.fit(X_tr_w, y_tr, sample_weight=weights)\n",
    "    proba0_w = rf_w.predict_proba(X_te_w)[:, list(rf_w.classes_).index(0)]\n",
    "    met_w = evaluate_pos0(y_te, proba0_w)\n",
    "\n",
    "    # 6.3 RF + PS-guided resampling\n",
    "    X_tr_rs, y_tr_rs = ps_guided_resample(X_tr_w, y_tr, phero_tr, target_ratio=target_ratio, alpha=alpha, beta=beta, seed=SEED)\n",
    "    rf_rs = RandomForestClassifier(\n",
    "        n_estimators=rf_n_estimators,\n",
    "        random_state=rf_random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_rs.fit(X_tr_rs, y_tr_rs)\n",
    "    proba0_rs = rf_rs.predict_proba(X_te_w)[:, list(rf_rs.classes_).index(0)]\n",
    "    met_rs = evaluate_pos0(y_te, proba0_rs)\n",
    "\n",
    "    return met_base, met_w, met_rs\n",
    "\n",
    "# Attach index id to folds\n",
    "for i in range(len(folds)):\n",
    "    folds[i][\"id\"] = i\n",
    "\n",
    "all_rows = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_values:\n",
    "    base_list, w_list, rs_list = [], [], []\n",
    "    for fpack in folds:\n",
    "        m_base, m_w, m_rs = run_fold_strategies(fpack, k, rf_n_estimators)\n",
    "        base_list.append(m_base)\n",
    "        w_list.append(m_w)\n",
    "        rs_list.append(m_rs)\n",
    "\n",
    "    def avg(metrics_list):\n",
    "        keys = list(metrics_list[0].keys())\n",
    "        return {f\"{k}_mean\": float(np.mean([m[k] for m in metrics_list]))\n",
    "                for k in keys} | {f\"{k}_std\": float(np.std([m[k] for m in metrics_list]))\n",
    "                for k in keys}\n",
    "\n",
    "    res_base = {\"strategy\": \"rf_baseline\", \"k\": k} | avg(base_list)\n",
    "    res_w    = {\"strategy\": \"rf_weight\",   \"k\": k} | avg(w_list)\n",
    "    res_rs   = {\"strategy\": \"rf_resample\", \"k\": k} | avg(rs_list)\n",
    "    all_rows.extend([res_base, res_w, res_rs])\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "results_df = pd.DataFrame(all_rows)\n",
    "\n",
    "# =========================\n",
    "# 7) Report results\n",
    "# =========================\n",
    "show_cols = [\"strategy\", \"k\",\n",
    "             \"F1_mean\", \"Precision_mean\", \"Recall_mean\",\n",
    "             \"AUC_PR_mean\", \"ROC_AUC_mean\", \"Accuracy_mean\"]\n",
    "\n",
    "print(f\"[Light run] time: {t1 - t0:.1f}s, folds={N_FOLDS}, k_values={k_values}, \"\n",
    "      f\"cap_maj_for_graph_per_fold={cap_maj_for_graph_per_fold}\")\n",
    "\n",
    "print(\"\\n=== Summary (means over folds) ===\")\n",
    "def pr(df):\n",
    "    print(df[show_cols].to_string(index=False))\n",
    "\n",
    "pr(results_df.sort_values([\"strategy\",\"k\"]))\n",
    "\n",
    "# Optional: save results\n",
    "# results_df.to_csv(\"light_ps_results.csv\", index=False)\n"
   ],
   "id": "498c8bc3bdb0322c"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T11:46:40.550135Z",
     "start_time": "2025-09-07T11:46:39.312665Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T11:46:40.562188Z",
     "start_time": "2025-09-07T11:46:40.555139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Class'] = data.target\n",
    "\n",
    "features = data.feature_names.tolist()\n",
    "X = df[features].values\n",
    "y = df['Class'].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, np.arange(len(df)),\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)"
   ],
   "id": "ce225d2fceca9165",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T11:46:40.569315Z",
     "start_time": "2025-09-07T11:46:40.565617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def run_aco_with_knn(\n",
    "    X_train, y_train, X_test,\n",
    "    k=5, n_ants=50, n_iter=30, rho=0.1, rng_seed=42,\n",
    "    alpha_edge=1.0, beta_phero=1.0, tau=1.0,\n",
    "    max_steps=100, eps=1e-12\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Pheromone-Score (PS) via kNN graph + ACO on TRAIN only (no leakage),\n",
    "    then estimate PS for TEST by neighbor-weighted averaging.\n",
    "\n",
    "    Assumptions:\n",
    "      - X_train / X_test are already standardized (e.g., StandardScaler fit on train).\n",
    "\n",
    "    Parameters:\n",
    "      k (int): number of neighbors for kNN graph (on TRAIN).\n",
    "      n_ants (int): ants per iteration.\n",
    "      n_iter (int): number of ACO iterations.\n",
    "      rho (float): pheromone evaporation rate in (0,1).\n",
    "      rng_seed (int): random seed for reproducibility.\n",
    "      alpha_edge (float): exponent on edge weight (similarity emphasis).\n",
    "      beta_phero (float): exponent on (1 + pheromone) (exploitation emphasis).\n",
    "      tau (float): softmax temperature (lower → sharper selection).\n",
    "      max_steps (int): maximum steps per ant walk (prevents overly long paths).\n",
    "      eps (float): numerical stability constant.\n",
    "\n",
    "    Returns:\n",
    "      pheromone_train_scaled (np.ndarray): PS in [0,1] for TRAIN nodes (shape: [n_train]).\n",
    "      pheromone_test_scaled  (np.ndarray): PS estimate in [0,1] for TEST nodes  (shape: [n_test]).\n",
    "    \"\"\"\n",
    "    # kNN on TRAIN\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(X_train)\n",
    "    dist_train, ind_train = nbrs.kneighbors(X_train)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    n_tr = len(X_train)\n",
    "    for i in range(n_tr):\n",
    "        G.add_node(i, label=int(y_train[i]), pheromone=0.0)\n",
    "\n",
    "    # build weighted edges using returned distances (avoid recomputing norms)\n",
    "    for i, neigh in enumerate(ind_train):\n",
    "        for jj, j in enumerate(neigh):\n",
    "            if i == j:\n",
    "                continue\n",
    "            d_ij = dist_train[i, jj]\n",
    "            w = 1.0 / (1.0 + d_ij)\n",
    "            G.add_edge(i, j, weight=w)\n",
    "\n",
    "    majority_nodes = [i for i in range(n_tr) if y_train[i] == 1]\n",
    "    minority_nodes = set([i for i in range(n_tr) if y_train[i] == 0])\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # ACO loop\n",
    "    for _ in range(n_iter):\n",
    "        for _ in range(n_ants):\n",
    "            current = rng.choice(majority_nodes)\n",
    "            visited = [current]\n",
    "            steps = 0\n",
    "\n",
    "            while steps < max_steps:\n",
    "                neighs = [n for n in G.neighbors(current) if n not in visited]\n",
    "                if not neighs:\n",
    "                    break\n",
    "\n",
    "                # move probability ∝ (edge_weight^alpha_edge) * ((1+pheromone)^beta_phero)\n",
    "                raw = []\n",
    "                for n in neighs:\n",
    "                    e = G[current][n]['weight'] ** alpha_edge\n",
    "                    p = (1.0 + G.nodes[n]['pheromone']) ** beta_phero\n",
    "                    raw.append(e * p)\n",
    "                raw = np.array(raw, dtype=float)\n",
    "\n",
    "                # temperature-scaled softmax for stability/controllability\n",
    "                logits = raw / max(tau, eps)\n",
    "                logits -= logits.max()  # numerical stability\n",
    "                probs = np.exp(logits)\n",
    "                s = probs.sum()\n",
    "                if s <= eps:\n",
    "                    break\n",
    "                probs /= s\n",
    "\n",
    "                next_node = rng.choice(neighs, p=probs)\n",
    "                visited.append(next_node)\n",
    "                current = next_node\n",
    "                steps += 1\n",
    "\n",
    "                if current in minority_nodes:\n",
    "                    for idx in visited:\n",
    "                        G.nodes[idx]['pheromone'] += 1.0\n",
    "                    break\n",
    "\n",
    "        # evaporation\n",
    "        for n in G.nodes:\n",
    "            G.nodes[n]['pheromone'] *= (1.0 - rho)\n",
    "\n",
    "    # normalize PS on TRAIN\n",
    "    pheromone_train = np.array([G.nodes[i]['pheromone'] for i in range(n_tr)], dtype=float)\n",
    "    scaler = MinMaxScaler().fit(pheromone_train.reshape(-1, 1))\n",
    "    pheromone_train_scaled = scaler.transform(pheromone_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # estimate PS for TEST via inverse-distance weighted average of TRAIN neighbors\n",
    "    dist_te, ind_te = nbrs.kneighbors(X_test, n_neighbors=k)\n",
    "    w_te = 1.0 / (eps + dist_te)\n",
    "    w_te = w_te / (w_te.sum(axis=1, keepdims=True) + eps)\n",
    "    pheromone_test_scaled = (w_te * pheromone_train_scaled[ind_te]).sum(axis=1)\n",
    "\n",
    "    return pheromone_train_scaled, pheromone_test_scaled\n"
   ],
   "id": "d573440a084987db",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T12:03:05.761274Z",
     "start_time": "2025-09-07T11:46:40.572299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    average_precision_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Assumption: X, y and the function run_aco_with_knn are already defined\n",
    "# y: 0 = minority, 1 = majority\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ======= Parameter space =======\n",
    "# ACO/kNN\n",
    "k_values            = [5, 10]\n",
    "n_ants_values       = [50, 100]\n",
    "n_iter_values       = [20, 30]\n",
    "rho_values          = [0.1, 0.3]\n",
    "\n",
    "# ACO movement policy\n",
    "alpha_edge_values   = [0.5, 1.0, 2.0]\n",
    "beta_phero_values   = [0.5, 1.0, 2.0]\n",
    "tau_values          = [0.3, 1.0, 3.0]\n",
    "\n",
    "# Training sample weighting\n",
    "alpha_values        = [1.0]\n",
    "beta_values         = [1.0, 2.0]\n",
    "gamma_values        = [1.0]\n",
    "delta_values        = [0.0, 0.5]\n",
    "\n",
    "# K-Fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare fold data (scaling applied on train only)\n",
    "# ----------------------------\n",
    "fold_data = []  # [(fold, X_tr, y_tr, X_te, y_te)]\n",
    "for f, (tr_idx, te_idx) in enumerate(cv.split(X, y)):\n",
    "    X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "    scaler = StandardScaler().fit(X_tr_raw)\n",
    "    X_tr = scaler.transform(X_tr_raw)\n",
    "    X_te = scaler.transform(X_te_raw)\n",
    "    fold_data.append((f, X_tr, y_tr, X_te, y_te))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) PRECOMPUTE PS with parallelization\n",
    "# ----------------------------\n",
    "aco_grid = list(product(\n",
    "    k_values, n_ants_values, n_iter_values, rho_values,\n",
    "    alpha_edge_values, beta_phero_values, tau_values\n",
    "))\n",
    "\n",
    "def compute_ps_for_combo(combo):\n",
    "    # Each combo is evaluated across all folds\n",
    "    k, n_ants, n_iter, rho, alpha_edge, beta_phero, tau = combo\n",
    "    outs = []\n",
    "    for (f, X_tr, y_tr, X_te, y_te) in fold_data:\n",
    "        # Shorten ant paths → faster execution\n",
    "        p_tr, p_te = run_aco_with_knn(\n",
    "            X_tr, y_tr, X_te,\n",
    "            k=k, n_ants=n_ants, n_iter=n_iter, rho=rho, rng_seed=42,\n",
    "            alpha_edge=alpha_edge, beta_phero=beta_phero, tau=tau,\n",
    "            max_steps=15*k\n",
    "        )\n",
    "        outs.append((f, p_tr, p_te))\n",
    "    return combo, outs\n",
    "\n",
    "n_jobs = max(2, min(8, (os.cpu_count() or 4)//2))\n",
    "t0 = time.perf_counter()\n",
    "results = Parallel(n_jobs=n_jobs, prefer=\"processes\", verbose=10)(\n",
    "    delayed(compute_ps_for_combo)(combo) for combo in aco_grid\n",
    ")\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# Build cache\n",
    "ps_cache = {}\n",
    "for combo, outs in results:\n",
    "    k, n_ants, n_iter, rho, alpha_edge, beta_phero, tau = combo\n",
    "    for (f, p_tr, p_te) in outs:\n",
    "        ps_cache[(f, k, n_ants, n_iter, rho, alpha_edge, beta_phero, tau)] = (p_tr, p_te)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Evaluate weightings using precomputed PS\n",
    "# ----------------------------\n",
    "def eval_with_cached_ps(k, n_ants, n_iter, rho,\n",
    "                        alpha_edge, beta_phero, tau,\n",
    "                        alpha, beta, gamma, delta,\n",
    "                        rf_n_estimators=100):\n",
    "    f1_m_list, prec_m_list, rec_m_list = [], [], []\n",
    "    macro_f1_list, acc_list = [], []\n",
    "    aucpr_m_list, aucroc_m_list = [], []\n",
    "    fit_time_list, pred_time_list = [], []\n",
    "\n",
    "    for (f, X_tr, y_tr, X_te, y_te) in fold_data:\n",
    "        phero_tr, phero_te = ps_cache[(f, k, n_ants, n_iter, rho, alpha_edge, beta_phero, tau)]\n",
    "        X_tr_phero = np.column_stack([X_tr, phero_tr])\n",
    "        X_te_phero = np.column_stack([X_te, phero_te])\n",
    "\n",
    "        # Sample weighting\n",
    "        w_tr = np.where(\n",
    "            y_tr == 1,\n",
    "            gamma + delta * phero_tr,\n",
    "            alpha + beta  * phero_tr\n",
    "        )\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=rf_n_estimators,\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "        t_fit0 = time.perf_counter()\n",
    "        clf.fit(X_tr_phero, y_tr, sample_weight=w_tr)\n",
    "        t_fit1 = time.perf_counter()\n",
    "\n",
    "        t_pred0 = time.perf_counter()\n",
    "        proba1 = clf.predict_proba(X_te_phero)[:, 1]\n",
    "        y_pred = (proba1 >= 0.5).astype(int)\n",
    "        t_pred1 = time.perf_counter()\n",
    "\n",
    "        # Metrics for minority class = 0\n",
    "        f1_m   = f1_score(y_te, y_pred, pos_label=0)\n",
    "        prec_m = precision_score(y_te, y_pred, pos_label=0, zero_division=0)\n",
    "        rec_m  = recall_score(y_te, y_pred, pos_label=0)\n",
    "        macroF1 = f1_score(y_te, y_pred, average='macro')\n",
    "        acc    = accuracy_score(y_te, y_pred)\n",
    "\n",
    "        proba_min = 1.0 - proba1\n",
    "        aucpr_m   = average_precision_score(1 - y_te, proba_min)\n",
    "        aucroc_m  = roc_auc_score(1 - y_te, proba_min)\n",
    "\n",
    "        f1_m_list.append(f1_m)\n",
    "        prec_m_list.append(prec_m)\n",
    "        rec_m_list.append(rec_m)\n",
    "        macro_f1_list.append(macroF1)\n",
    "        acc_list.append(acc)\n",
    "        aucpr_m_list.append(aucpr_m)\n",
    "        aucroc_m_list.append(aucroc_m)\n",
    "        fit_time_list.append(t_fit1 - t_fit0)\n",
    "        pred_time_list.append(t_pred1 - t_pred0)\n",
    "\n",
    "    return {\n",
    "        \"k\": k, \"n_ants\": n_ants, \"n_iter\": n_iter, \"rho\": rho,\n",
    "        \"alpha_edge\": alpha_edge, \"beta_phero\": beta_phero, \"tau\": tau,\n",
    "        \"alpha\": alpha, \"beta\": beta, \"gamma\": gamma, \"delta\": delta,\n",
    "        \"F1_minority_mean\":   float(np.mean(f1_m_list)),\n",
    "        \"F1_minority_std\":    float(np.std(f1_m_list)),\n",
    "        \"Precision_min_mean\": float(np.mean(prec_m_list)),\n",
    "        \"Recall_min_mean\":    float(np.mean(rec_m_list)),\n",
    "        \"MacroF1_mean\":       float(np.mean(macro_f1_list)),\n",
    "        \"Accuracy_mean\":      float(np.mean(acc_list)),\n",
    "        \"AUC_PR_min_mean\":    float(np.mean(aucpr_m_list)),\n",
    "        \"AUC_PR_min_std\":     float(np.std(aucpr_m_list)),\n",
    "        \"AUC_ROC_min_mean\":   float(np.mean(aucroc_m_list)),\n",
    "        \"AUC_ROC_min_std\":    float(np.std(aucroc_m_list)),\n",
    "        \"TrainTime_s_mean\":   float(np.mean(fit_time_list)),\n",
    "        \"PredTime_s_mean\":    float(np.mean(pred_time_list)),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "start_eval = time.perf_counter()\n",
    "for (k, n_ants, n_iter, rho, alpha_edge, beta_phero, tau) in aco_grid:\n",
    "    for (alpha, beta, gamma, delta) in product(alpha_values, beta_values, gamma_values, delta_values):\n",
    "        rows.append(\n",
    "            eval_with_cached_ps(k, n_ants, n_iter, rho,\n",
    "                                alpha_edge, beta_phero, tau,\n",
    "                                alpha, beta, gamma, delta)\n",
    "        )\n",
    "end_eval = time.perf_counter()\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# ----------------------------\n",
    "# Final report\n",
    "# ----------------------------\n",
    "print(f\"[PS Precompute] combos: {len(aco_grid)}, n_jobs={n_jobs}, time: {t1 - t0:.2f}s\")\n",
    "print(f\"[RF Eval] combos: {len(rows)}, time: {end_eval - start_eval:.2f}s\")\n",
    "\n",
    "topN = 15\n",
    "show_cols = [\n",
    "    \"k\",\"n_ants\",\"n_iter\",\"rho\",\"alpha_edge\",\"beta_phero\",\"tau\",\n",
    "    \"alpha\",\"beta\",\"gamma\",\"delta\",\n",
    "    \"F1_minority_mean\",\"F1_minority_std\",\n",
    "    \"AUC_PR_min_mean\",\"AUC_PR_min_std\",\n",
    "    \"Precision_min_mean\",\"Recall_min_mean\",\n",
    "    \"MacroF1_mean\",\"Accuracy_mean\",\n",
    "    \"TrainTime_s_mean\",\"PredTime_s_mean\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Top by F1_minority_mean ===\")\n",
    "print(results_df.sort_values(\"F1_minority_mean\", ascending=False)[show_cols].head(topN))\n",
    "\n",
    "print(\"\\n=== Top by AUC_PR_min_mean ===\")\n",
    "print(results_df.sort_values(\"AUC_PR_min_mean\", ascending=False)[show_cols].head(topN))\n",
    "\n",
    "# results_df.to_csv(\"ps_weighted_full_grid_results.csv\", index=False)\n"
   ],
   "id": "9a4c6b8854925e0d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=6)]: Done 253 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done 301 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=6)]: Done 326 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=6)]: Done 409 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=6)]: Done 432 out of 432 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PS Precompute] combos: 432, n_jobs=6, time: 447.78s\n",
      "[RF Eval] combos: 1728, time: 537.35s\n",
      "\n",
      "=== Top by F1_minority_mean ===\n",
      "     k  n_ants  n_iter  rho  alpha_edge  beta_phero  tau  alpha  beta  gamma  \\\n",
      "513  5     100      20  0.1         2.0         0.5  3.0    1.0   1.0    1.0   \n",
      "93   5      50      20  0.1         2.0         1.0  3.0    1.0   1.0    1.0   \n",
      "418  5      50      30  0.3         2.0         1.0  3.0    1.0   2.0    1.0   \n",
      "515  5     100      20  0.1         2.0         0.5  3.0    1.0   2.0    1.0   \n",
      "186  5      50      20  0.3         2.0         0.5  1.0    1.0   2.0    1.0   \n",
      "239  5      50      30  0.1         0.5         1.0  3.0    1.0   2.0    1.0   \n",
      "512  5     100      20  0.1         2.0         0.5  3.0    1.0   1.0    1.0   \n",
      "731  5     100      30  0.1         2.0         0.5  3.0    1.0   2.0    1.0   \n",
      "730  5     100      30  0.1         2.0         0.5  3.0    1.0   2.0    1.0   \n",
      "729  5     100      30  0.1         2.0         0.5  3.0    1.0   1.0    1.0   \n",
      "621  5     100      20  0.3         2.0         0.5  3.0    1.0   1.0    1.0   \n",
      "238  5      50      30  0.1         0.5         1.0  3.0    1.0   2.0    1.0   \n",
      "153  5      50      20  0.3         1.0         0.5  3.0    1.0   1.0    1.0   \n",
      "617  5     100      20  0.3         2.0         0.5  1.0    1.0   1.0    1.0   \n",
      "659  5     100      30  0.1         0.5         0.5  3.0    1.0   2.0    1.0   \n",
      "\n",
      "     ...  F1_minority_mean  F1_minority_std  AUC_PR_min_mean  AUC_PR_min_std  \\\n",
      "513  ...          0.955712         0.025847         0.989117        0.007819   \n",
      "93   ...          0.955601         0.025758         0.990624        0.005737   \n",
      "418  ...          0.953656         0.016743         0.988048        0.008243   \n",
      "515  ...          0.953653         0.024091         0.988605        0.007687   \n",
      "186  ...          0.953304         0.024290         0.990016        0.006962   \n",
      "239  ...          0.951198         0.020764         0.990094        0.006472   \n",
      "512  ...          0.951061         0.023643         0.989272        0.007578   \n",
      "731  ...          0.951006         0.021227         0.989657        0.007269   \n",
      "730  ...          0.951006         0.021227         0.988244        0.006888   \n",
      "729  ...          0.950957         0.023919         0.989480        0.006313   \n",
      "621  ...          0.950957         0.023919         0.988627        0.006589   \n",
      "238  ...          0.950933         0.019341         0.990738        0.005260   \n",
      "153  ...          0.950895         0.021093         0.988779        0.006939   \n",
      "617  ...          0.950883         0.026817         0.988749        0.006469   \n",
      "659  ...          0.950725         0.020984         0.987780        0.006951   \n",
      "\n",
      "     Precision_min_mean  Recall_min_mean  MacroF1_mean  Accuracy_mean  \\\n",
      "513            0.985646         0.929347      0.965544       0.968359   \n",
      "93             0.989975         0.924585      0.965508       0.968359   \n",
      "418            0.980918         0.929236      0.963770       0.966605   \n",
      "515            0.981100         0.929347      0.963777       0.966620   \n",
      "186            0.985435         0.924585      0.963651       0.966605   \n",
      "239            0.975674         0.929125      0.961851       0.964835   \n",
      "512            0.981100         0.924585      0.961810       0.964850   \n",
      "731            0.980783         0.924585      0.961792       0.964850   \n",
      "730            0.980783         0.924585      0.961792       0.964850   \n",
      "729            0.980196         0.924585      0.961768       0.964835   \n",
      "621            0.980196         0.924585      0.961768       0.964835   \n",
      "238            0.981501         0.924363      0.961764       0.964850   \n",
      "153            0.985213         0.919934      0.961757       0.964850   \n",
      "617            0.980641         0.924695      0.961758       0.964866   \n",
      "659            0.989859         0.915172      0.961700       0.964850   \n",
      "\n",
      "     TrainTime_s_mean  PredTime_s_mean  \n",
      "513          0.059704         0.001060  \n",
      "93           0.059734         0.001076  \n",
      "418          0.059956         0.001069  \n",
      "515          0.060098         0.001079  \n",
      "186          0.058563         0.001053  \n",
      "239          0.059969         0.001057  \n",
      "512          0.059126         0.001058  \n",
      "731          0.059385         0.001081  \n",
      "730          0.058128         0.001061  \n",
      "729          0.059695         0.001097  \n",
      "621          0.060369         0.001074  \n",
      "238          0.059390         0.001075  \n",
      "153          0.060858         0.001066  \n",
      "617          0.060496         0.001064  \n",
      "659          0.059701         0.001072  \n",
      "\n",
      "[15 rows x 21 columns]\n",
      "\n",
      "=== Top by AUC_PR_min_mean ===\n",
      "       k  n_ants  n_iter  rho  alpha_edge  beta_phero  tau  alpha  beta  \\\n",
      "853    5     100      30  0.3         2.0         2.0  0.3    1.0   1.0   \n",
      "1131  10      50      30  0.1         1.0         1.0  0.3    1.0   2.0   \n",
      "1130  10      50      30  0.1         1.0         1.0  0.3    1.0   2.0   \n",
      "381    5      50      30  0.3         1.0         1.0  3.0    1.0   1.0   \n",
      "1564  10     100      30  0.1         1.0         1.0  1.0    1.0   1.0   \n",
      "1548  10     100      30  0.1         1.0         0.5  0.3    1.0   1.0   \n",
      "950   10      50      20  0.1         2.0         1.0  0.3    1.0   2.0   \n",
      "213    5      50      20  0.3         2.0         2.0  3.0    1.0   1.0   \n",
      "488    5     100      20  0.1         1.0         1.0  3.0    1.0   1.0   \n",
      "1162  10      50      30  0.1         2.0         0.5  3.0    1.0   2.0   \n",
      "704    5     100      30  0.1         1.0         1.0  3.0    1.0   1.0   \n",
      "770    5     100      30  0.3         0.5         1.0  0.3    1.0   2.0   \n",
      "1655  10     100      30  0.3         0.5         2.0  3.0    1.0   2.0   \n",
      "1102  10      50      30  0.1         0.5         1.0  3.0    1.0   2.0   \n",
      "1549  10     100      30  0.1         1.0         0.5  0.3    1.0   1.0   \n",
      "\n",
      "      gamma  ...  F1_minority_mean  F1_minority_std  AUC_PR_min_mean  \\\n",
      "853     1.0  ...          0.928295         0.032256         0.991874   \n",
      "1131    1.0  ...          0.924786         0.045278         0.991560   \n",
      "1130    1.0  ...          0.935673         0.043830         0.991546   \n",
      "381     1.0  ...          0.940926         0.024731         0.991396   \n",
      "1564    1.0  ...          0.933814         0.030657         0.991373   \n",
      "1548    1.0  ...          0.930894         0.044571         0.991300   \n",
      "950     1.0  ...          0.940068         0.024046         0.991207   \n",
      "213     1.0  ...          0.938195         0.023556         0.991205   \n",
      "488     1.0  ...          0.937543         0.029811         0.991094   \n",
      "1162    1.0  ...          0.942852         0.023978         0.991067   \n",
      "704     1.0  ...          0.939941         0.032048         0.991040   \n",
      "770     1.0  ...          0.934120         0.032631         0.991026   \n",
      "1655    1.0  ...          0.917962         0.036667         0.990995   \n",
      "1102    1.0  ...          0.936653         0.040671         0.990985   \n",
      "1549    1.0  ...          0.933204         0.040990         0.990919   \n",
      "\n",
      "      AUC_PR_min_std  Precision_min_mean  Recall_min_mean  MacroF1_mean  \\\n",
      "853         0.004239            1.000000         0.867885      0.945406   \n",
      "1131        0.005561            1.000000         0.863344      0.943109   \n",
      "1130        0.005371            1.000000         0.882281      0.951142   \n",
      "381         0.004555            0.975990         0.910410      0.954057   \n",
      "1564        0.005474            1.000000         0.877409      0.949445   \n",
      "1548        0.005525            0.994118         0.877409      0.947385   \n",
      "950         0.004276            0.989975         0.896124      0.953758   \n",
      "213         0.004995            0.975980         0.905648      0.952047   \n",
      "488         0.006786            0.984804         0.896346      0.951825   \n",
      "1162        0.005135            0.985435         0.905648      0.955776   \n",
      "704         0.005351            0.985071         0.900886      0.953713   \n",
      "770         0.005532            0.994595         0.881949      0.949560   \n",
      "1655        0.005547            0.994118         0.853931      0.937598   \n",
      "1102        0.004985            0.989524         0.891694      0.951507   \n",
      "1549        0.004686            0.995455         0.882060      0.949235   \n",
      "\n",
      "      Accuracy_mean  TrainTime_s_mean  PredTime_s_mean  \n",
      "853        0.950784          0.057179         0.001056  \n",
      "1131       0.949045          0.059397         0.001151  \n",
      "1130       0.956063          0.070991         0.001183  \n",
      "381        0.957833          0.061762         0.001101  \n",
      "1564       0.954293          0.055813         0.001087  \n",
      "1548       0.952569          0.057861         0.001071  \n",
      "950        0.957833          0.059171         0.001048  \n",
      "213        0.956078          0.060165         0.001059  \n",
      "488        0.956078          0.058945         0.001063  \n",
      "1162       0.959571          0.061654         0.001136  \n",
      "704        0.957833          0.059034         0.001045  \n",
      "770        0.954308          0.056091         0.001040  \n",
      "1655       0.943798          0.058651         0.001136  \n",
      "1102       0.956078          0.057193         0.001059  \n",
      "1549       0.954324          0.058820         0.001070  \n",
      "\n",
      "[15 rows x 21 columns]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T12:05:46.932761Z",
     "start_time": "2025-09-07T12:05:41.285203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data: Breast Cancer (0 = malignant, minority; 1 = benign, majority)\n",
    "# -----------------------------\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # 0 = minority (malignant), 1 = majority (benign)\n",
    "\n",
    "# -----------------------------\n",
    "# Function: ACO on kNN graph only on TRAIN (no leakage)\n",
    "# Estimate pheromone for TEST from TRAIN neighbors\n",
    "# Movement policy: softmax with temperature τ and weighting (edge^alpha_edge) * (1+pheromone)^beta_phero\n",
    "# -----------------------------\n",
    "def run_aco_with_knn(\n",
    "    X_train, y_train, X_test,\n",
    "    k=5, n_ants=100, n_iter=20, rho=0.1, rng_seed=42,\n",
    "    alpha_edge=1.0, beta_phero=1.0, tau=1.0,\n",
    "    max_steps=None, eps=1e-12\n",
    "):\n",
    "    # kNN based on TRAIN\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(X_train)\n",
    "    dist_train, ind_train = nbrs.kneighbors(X_train)\n",
    "\n",
    "    # Weighted undirected graph\n",
    "    G = nx.Graph()\n",
    "    n_tr = X_train.shape[0]\n",
    "    for i in range(n_tr):\n",
    "        G.add_node(i, label=int(y_train[i]), pheromone=0.0)\n",
    "\n",
    "    # Edges with weight 1/(1+d) (using kneighbors for efficiency)\n",
    "    for i, neigh in enumerate(ind_train):\n",
    "        for jj, j in enumerate(neigh):\n",
    "            if i == j:\n",
    "                continue\n",
    "            d_ij = dist_train[i, jj]\n",
    "            w = 1.0 / (1.0 + d_ij)\n",
    "            G.add_edge(i, j, weight=w)\n",
    "\n",
    "    majority_nodes = [i for i in range(n_tr) if y_train[i] == 1]\n",
    "    minority_nodes = set([i for i in range(n_tr) if y_train[i] == 0])\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    if max_steps is None:\n",
    "        max_steps = 3 * k  # reasonable cap for speed\n",
    "\n",
    "    # ACO iterations\n",
    "    for _ in range(n_iter):\n",
    "        for _ in range(n_ants):\n",
    "            current = rng.choice(majority_nodes)\n",
    "            visited = [current]\n",
    "            steps = 0\n",
    "\n",
    "            while steps < max_steps:\n",
    "                neighs = [n for n in G.neighbors(current) if n not in visited]\n",
    "                if not neighs:\n",
    "                    break\n",
    "\n",
    "                # Move probability ∝ (edge_weight^alpha_edge) * ((1+pheromone)^beta_phero)\n",
    "                raw = []\n",
    "                for n in neighs:\n",
    "                    e = G[current][n]['weight'] ** alpha_edge\n",
    "                    p = (1.0 + G.nodes[n]['pheromone']) ** beta_phero\n",
    "                    raw.append(e * p)\n",
    "                raw = np.asarray(raw, dtype=float)\n",
    "\n",
    "                # Softmax with temperature τ\n",
    "                logits = raw / max(tau, eps)\n",
    "                logits -= logits.max()\n",
    "                probs = np.exp(logits)\n",
    "                s = probs.sum()\n",
    "                if s <= eps:\n",
    "                    break\n",
    "                probs /= s\n",
    "\n",
    "                next_node = rng.choice(neighs, p=probs)\n",
    "                visited.append(next_node)\n",
    "                current = next_node\n",
    "                steps += 1\n",
    "\n",
    "                # Reaching a minority node → reinforce the whole path\n",
    "                if current in minority_nodes:\n",
    "                    for idx in visited:\n",
    "                        G.nodes[idx]['pheromone'] += 1.0\n",
    "                    break\n",
    "\n",
    "        # Pheromone evaporation\n",
    "        for n in G.nodes:\n",
    "            G.nodes[n]['pheromone'] *= (1.0 - rho)\n",
    "\n",
    "    # Normalize PS on TRAIN\n",
    "    phero_tr = np.array([G.nodes[i]['pheromone'] for i in range(n_tr)], dtype=float)\n",
    "    phero_scaler = MinMaxScaler().fit(phero_tr.reshape(-1, 1))\n",
    "    phero_tr_s = phero_scaler.transform(phero_tr.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Estimate PS for TEST using inverse-distance weighted average from TRAIN\n",
    "    dist_te, ind_te = nbrs.kneighbors(X_test, n_neighbors=k)\n",
    "    w_te = 1.0 / (eps + dist_te)\n",
    "    w_te = w_te / (w_te.sum(axis=1, keepdims=True) + eps)\n",
    "    phero_te_s = (w_te * phero_tr_s[ind_te]).sum(axis=1)\n",
    "\n",
    "    return phero_tr_s, phero_te_s\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Minority metrics (class 0)\n",
    "# -----------------------------\n",
    "def minority_metrics_from_proba(y_true, proba_class1):\n",
    "    # For minority = 0, probability of minority = 1 - proba_class1\n",
    "    proba_minority = 1.0 - proba_class1\n",
    "    y_pred = (proba_class1 >= 0.5).astype(int)  # 1 = majority\n",
    "    f1_min = f1_score(y_true, y_pred, pos_label=0)\n",
    "    aucpr_min = average_precision_score(1 - y_true, proba_minority)\n",
    "    return f1_min, aucpr_min\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate our method (two modes: PS feature only / PS feature + weighting)\n",
    "# -----------------------------\n",
    "def eval_my_method(\n",
    "    X, y, cv,\n",
    "    # ACO/kNN\n",
    "    k=5, n_ants=50, n_iter=30, rho=0.3,\n",
    "    alpha_edge=2.0, beta_phero=0.5, tau=3.0, max_steps=None,\n",
    "    # Training weighting\n",
    "    alpha=1.0, beta=2.0, gamma=1.0, delta=0.5,\n",
    "    use_weights=True, random_state=42\n",
    "):\n",
    "    f1s, aucprs = [], []\n",
    "\n",
    "    for tr_idx, te_idx in cv.split(X, y):\n",
    "        X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_tr_raw)\n",
    "        X_tr = scaler.transform(X_tr_raw)\n",
    "        X_te = scaler.transform(X_te_raw)\n",
    "\n",
    "        # PS computed only from TRAIN\n",
    "        phero_tr, phero_te = run_aco_with_knn(\n",
    "            X_tr, y_tr, X_te,\n",
    "            k=k, n_ants=n_ants, n_iter=n_iter, rho=rho, rng_seed=random_state,\n",
    "            alpha_edge=alpha_edge, beta_phero=beta_phero, tau=tau, max_steps=max_steps\n",
    "        )\n",
    "\n",
    "        X_tr_phero = np.column_stack([X_tr, phero_tr])\n",
    "        X_te_phero = np.column_stack([X_te, phero_te])\n",
    "\n",
    "        sample_weight = None\n",
    "        if use_weights:\n",
    "            sample_weight = np.where(\n",
    "                y_tr == 1,\n",
    "                gamma + delta * phero_tr,      # Majority: hard negatives get extra weight\n",
    "                alpha + beta  * phero_tr       # Minority\n",
    "            )\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        clf.fit(X_tr_phero, y_tr, sample_weight=sample_weight)\n",
    "        proba1 = clf.predict_proba(X_te_phero)[:, 1]\n",
    "\n",
    "        f1_min, aucpr_min = minority_metrics_from_proba(y_te, proba1)\n",
    "        f1s.append(f1_min)\n",
    "        aucprs.append(aucpr_min)\n",
    "\n",
    "    return {\n",
    "        \"F1_minority_mean\": np.mean(f1s), \"F1_minority_std\": np.std(f1s),\n",
    "        \"AUC_PR_minority_mean\": np.mean(aucprs), \"AUC_PR_minority_std\": np.std(aucprs)\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Baselines (strong and weak)\n",
    "# -----------------------------\n",
    "def eval_baselines(X, y, cv, random_state=42):\n",
    "    rows = []\n",
    "\n",
    "    for tr_idx, te_idx in cv.split(X, y):\n",
    "        X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_tr_raw)\n",
    "        X_tr = scaler.transform(X_tr_raw)\n",
    "        X_te = scaler.transform(X_te_raw)\n",
    "\n",
    "        # Dummy: always predict majority\n",
    "        clf_dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "        clf_dummy.fit(X_tr, y_tr)\n",
    "        proba1 = clf_dummy.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"Dummy_MostFrequent\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Plain RF\n",
    "        rf_plain = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        rf_plain.fit(X_tr, y_tr)\n",
    "        proba1 = rf_plain.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_plain\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Logistic Regression without class_weight\n",
    "        logreg = LogisticRegression(max_iter=1000, solver=\"lbfgs\", random_state=random_state)\n",
    "        logreg.fit(X_tr, y_tr)\n",
    "        proba1 = logreg.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"LogReg_noCW\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Simple KNN\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, weights=\"uniform\")\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        proba1 = knn.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"KNN_k5_uniform\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Shallow Decision Tree\n",
    "        dt = DecisionTreeClassifier(max_depth=3, random_state=random_state)\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        proba1 = dt.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"DecisionTree_depth3\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # GaussianNB\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_tr, y_tr)\n",
    "        proba1 = gnb.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"GaussianNB\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Simple Under-Sampling + RF\n",
    "        X_res, y_res = RandomUnderSampler(random_state=random_state).fit_resample(X_tr, y_tr)\n",
    "        rf_under = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        rf_under.fit(X_res, y_res)\n",
    "        proba1 = rf_under.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_RandomUnderSampler\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Simple Over-Sampling + RF\n",
    "        X_res, y_res = RandomOverSampler(random_state=random_state).fit_resample(X_tr, y_tr)\n",
    "        rf_over = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        rf_over.fit(X_res, y_res)\n",
    "        proba1 = rf_over.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_RandomOverSampler\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # RF + class_weight\n",
    "        rf_cw = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=random_state)\n",
    "        rf_cw.fit(X_tr, y_tr)\n",
    "        proba1 = rf_cw.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_class_weight\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # RF + SMOTE\n",
    "        X_res, y_res = SMOTE(random_state=random_state).fit_resample(X_tr, y_tr)\n",
    "        rf_smote = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        rf_smote.fit(X_res, y_res)\n",
    "        proba1 = rf_smote.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_SMOTE\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # RF + ADASYN\n",
    "        X_res, y_res = ADASYN(random_state=random_state).fit_resample(X_tr, y_tr)\n",
    "        rf_ada = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        rf_ada.fit(X_res, y_res)\n",
    "        proba1 = rf_ada.predict_proba(X_te)[:, 1]\n",
    "        rows.append((\"RF_ADASYN\", *minority_metrics_from_proba(y_te, proba1)))\n",
    "\n",
    "        # Default XGB (no cost-sensitivity), positive = minority (label flipped)\n",
    "        y_tr_pos = (y_tr == 0).astype(int)\n",
    "        y_te_pos = (y_te == 0).astype(int)\n",
    "        xgb_def = XGBClassifier(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.1,\n",
    "            subsample=0.9, colsample_bytree=0.8,\n",
    "            scale_pos_weight=1.0, random_state=random_state,\n",
    "            tree_method=\"hist\", eval_metric=\"logloss\"\n",
    "        )\n",
    "        xgb_def.fit(X_tr, y_tr_pos)\n",
    "        proba_pos = xgb_def.predict_proba(X_te)[:, 1]  # Minority probability\n",
    "        aucpr_min = average_precision_score(y_te_pos, proba_pos)\n",
    "        y_pred_pos = (proba_pos >= 0.5).astype(int)\n",
    "        f1_min = f1_score(y_te_pos, y_pred_pos, pos_label=1)\n",
    "        rows.append((\"XGB_default_noCost\", f1_min, aucpr_min))\n",
    "\n",
    "        # Cost-sensitive XGB\n",
    "        neg = (y_tr_pos == 0).sum()\n",
    "        pos = (y_tr_pos == 1).sum()\n",
    "        spw = neg / max(pos, 1)\n",
    "        xgb_cs = XGBClassifier(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.1,\n",
    "            subsample=0.9, colsample_bytree=0.8,\n",
    "            scale_pos_weight=spw, random_state=random_state,\n",
    "            tree_method=\"hist\", eval_metric=\"logloss\"\n",
    "        )\n",
    "        xgb_cs.fit(X_tr, y_tr_pos)\n",
    "        proba_pos = xgb_cs.predict_proba(X_te)[:, 1]\n",
    "        aucpr_min = average_precision_score(y_te_pos, proba_pos)\n",
    "        y_pred_pos = (proba_pos >= 0.5).astype(int)\n",
    "        f1_min = f1_score(y_te_pos, y_pred_pos, pos_label=1)\n",
    "        rows.append((\"XGB_cost_sensitive\", f1_min, aucpr_min))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"Method\", \"F1_minority\", \"AUC_PR_minority\"])\n",
    "    summary = df.groupby(\"Method\").agg([\"mean\", \"std\"])\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run: Shared CV for all comparisons\n",
    "# -----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Best parameters from previous results ---\n",
    "# 1) Best-by-F1:\n",
    "bestF1_params = dict(\n",
    "    k=5, n_ants=50, n_iter=30, rho=0.3,\n",
    "    alpha_edge=2.0, beta_phero=0.5, tau=3.0, max_steps=15,   # 3*k\n",
    "    alpha=1.0, beta=2.0, gamma=1.0, delta=0.5, use_weights=True\n",
    ")\n",
    "\n",
    "# 2) Best-by-AUC-PR:\n",
    "bestAUC_params = dict(\n",
    "    k=5, n_ants=50, n_iter=20, rho=0.3,\n",
    "    alpha_edge=0.5, beta_phero=1.0, tau=3.0, max_steps=15,   # 3*k\n",
    "    alpha=1.0, beta=1.0, gamma=1.0, delta=0.0, use_weights=True  # Mild weighting\n",
    ")\n",
    "\n",
    "# --- Evaluate our method with best params ---\n",
    "my_bestF1 = eval_my_method(X, y, cv, **bestF1_params, random_state=42)\n",
    "my_bestAUC = eval_my_method(X, y, cv, **bestAUC_params, random_state=42)\n",
    "\n",
    "# --- Baselines ---\n",
    "baseline_summary = eval_baselines(X, y, cv, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Final summary side by side\n",
    "# -----------------------------\n",
    "rows = [\n",
    "    (\"MyMethod_bestF1\",\n",
    "     my_bestF1[\"F1_minority_mean\"],  my_bestF1[\"F1_minority_std\"],\n",
    "     my_bestF1[\"AUC_PR_minority_mean\"], my_bestF1[\"AUC_PR_minority_std\"]),\n",
    "    (\"MyMethod_bestAUC\",\n",
    "     my_bestAUC[\"F1_minority_mean\"],  my_bestAUC[\"F1_minority_std\"],\n",
    "     my_bestAUC[\"AUC_PR_minority_mean\"], my_bestAUC[\"AUC_PR_minority_std\"]),\n",
    "]\n",
    "\n",
    "my_df = pd.DataFrame(rows, columns=[\"Method\",\"F1_mean\",\"F1_std\",\"AUC_PR_mean\",\"AUC_PR_std\"])\n",
    "\n",
    "# baseline_summary has multi-level columns → flatten column names\n",
    "base_df = baseline_summary.copy()\n",
    "base_df.columns = ['_'.join(col) for col in base_df.columns]\n",
    "base_df = base_df.reset_index().rename(columns={\n",
    "    \"F1_minority_mean\":\"F1_mean\", \"F1_minority_std\":\"F1_std\",\n",
    "    \"AUC_PR_minority_mean\":\"AUC_PR_mean\", \"AUC_PR_minority_std\":\"AUC_PR_std\"\n",
    "})\n",
    "\n",
    "final_df = pd.concat([my_df, base_df[[\"Method\",\"F1_mean\",\"F1_std\",\"AUC_PR_mean\",\"AUC_PR_std\"]]],\n",
    "                     ignore_index=True)\n",
    "\n",
    "# Display two tables: sorted by F1 and by AUC-PR\n",
    "print(\"\\n=== Final Comparison (5-fold CV) — Sorted by F1_minority_mean ===\")\n",
    "print(final_df.sort_values(\"F1_mean\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "print(\"\\n=== Final Comparison (5-fold CV) — Sorted by AUC_PR_minority_mean ===\")\n",
    "print(final_df.sort_values(\"AUC_PR_mean\", ascending=False).reset_index(drop=True))\n"
   ],
   "id": "f586d3ee3b8b6069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comparison (5-fold CV) — Sorted by F1_minority_mean ===\n",
      "                   Method   F1_mean    F1_std  AUC_PR_mean  AUC_PR_std\n",
      "0             LogReg_noCW  0.963341  0.026824     0.994299    0.006417\n",
      "1               RF_ADASYN  0.954274  0.021915     0.990752    0.006211\n",
      "2      XGB_cost_sensitive  0.952720  0.009213     0.992082    0.005532\n",
      "3         RF_class_weight  0.952283  0.017532     0.989176    0.007341\n",
      "4      XGB_default_noCost  0.949963  0.015790     0.992246    0.006285\n",
      "5          KNN_k5_uniform  0.948669  0.028529     0.979702    0.018188\n",
      "6    RF_RandomOverSampler  0.948255  0.016989     0.989122    0.006902\n",
      "7                RF_SMOTE  0.946483  0.016655     0.988146    0.006860\n",
      "8        MyMethod_bestAUC  0.943912  0.020439     0.991301    0.005898\n",
      "9   RF_RandomUnderSampler  0.940006  0.025400     0.987397    0.008432\n",
      "10        MyMethod_bestF1  0.939245  0.014950     0.988174    0.006879\n",
      "11               RF_plain  0.938063  0.020793     0.986920    0.008406\n",
      "12             GaussianNB  0.904522  0.028449     0.971517    0.027439\n",
      "13    DecisionTree_depth3  0.897566  0.031480     0.889983    0.048507\n",
      "14     Dummy_MostFrequent  0.000000  0.000000     0.372582    0.004415\n",
      "\n",
      "=== Final Comparison (5-fold CV) — Sorted by AUC_PR_minority_mean ===\n",
      "                   Method   F1_mean    F1_std  AUC_PR_mean  AUC_PR_std\n",
      "0             LogReg_noCW  0.963341  0.026824     0.994299    0.006417\n",
      "1      XGB_default_noCost  0.949963  0.015790     0.992246    0.006285\n",
      "2      XGB_cost_sensitive  0.952720  0.009213     0.992082    0.005532\n",
      "3        MyMethod_bestAUC  0.943912  0.020439     0.991301    0.005898\n",
      "4               RF_ADASYN  0.954274  0.021915     0.990752    0.006211\n",
      "5         RF_class_weight  0.952283  0.017532     0.989176    0.007341\n",
      "6    RF_RandomOverSampler  0.948255  0.016989     0.989122    0.006902\n",
      "7         MyMethod_bestF1  0.939245  0.014950     0.988174    0.006879\n",
      "8                RF_SMOTE  0.946483  0.016655     0.988146    0.006860\n",
      "9   RF_RandomUnderSampler  0.940006  0.025400     0.987397    0.008432\n",
      "10               RF_plain  0.938063  0.020793     0.986920    0.008406\n",
      "11         KNN_k5_uniform  0.948669  0.028529     0.979702    0.018188\n",
      "12             GaussianNB  0.904522  0.028449     0.971517    0.027439\n",
      "13    DecisionTree_depth3  0.897566  0.031480     0.889983    0.048507\n",
      "14     Dummy_MostFrequent  0.000000  0.000000     0.372582    0.004415\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
